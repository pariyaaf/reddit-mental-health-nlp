{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد ردیف‌ها پس از فیلتر کردن بر اساس لیبل: 1611011\n",
      "انجام نمونه‌برداری با s_size=8000 برای هر کلاس...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8888\\168613719.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_filtered.groupby(label_column, group_keys=False).apply(lambda x: x.sample(min(len(x), s_size), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد ردیف‌ها پس از نمونه‌برداری: 40000\n",
      "\n",
      "توزیع نهایی داده‌ها بر اساس کلاس:\n",
      "subreddit\n",
      "anxiety         8000\n",
      "depression      8000\n",
      "lonely          8000\n",
      "mentalhealth    8000\n",
      "suicidewatch    8000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "MODEL_PATH = \"./my_bert_model\"\n",
    "DATA_PATH = \"dataset\"\n",
    "TEXT_COLUMN = \"selftext\"\n",
    "LABEL_COLUMN = \"subreddit\"\n",
    "\n",
    "def load_and_filter_dataset(data_path, text_column, label_column, s_size=None):\n",
    "\n",
    "    allowed_labels = ['suicidewatch', 'depression', 'lonely', 'mentalhealth', 'anxiety']\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(data_path, \"**\", \"*.csv\"), recursive=True)\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"هیچ فایل CSV در مسیر '{data_path}' پیدا نشد.\")\n",
    "    \n",
    "    all_dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "    all_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    clear_data = all_data.dropna(subset=[text_column, label_column]).copy()\n",
    "    clear_data[text_column] = clear_data[text_column].astype(str)\n",
    "    clear_data = clear_data[~clear_data[text_column].str.lower().isin(['[removed]', '[deleted]'])]\n",
    "    clear_data = clear_data[clear_data[text_column].str.strip() != '']\n",
    "    \n",
    "    df_filtered = clear_data[clear_data[label_column].str.lower().isin(allowed_labels)].copy()\n",
    "\n",
    "    df_filtered.loc[:, label_column] = df_filtered[label_column].str.lower()\n",
    "\n",
    "    print(f\"تعداد ردیف‌ها پس از فیلتر کردن بر اساس لیبل: {len(df_filtered)}\")\n",
    "    if df_filtered.empty:\n",
    "        return df_filtered\n",
    "\n",
    "    if s_size is not None:\n",
    "        print(f\"انجام نمونه‌برداری با s_size={s_size} برای هر کلاس...\")\n",
    "        df_sampled = df_filtered.groupby(label_column, group_keys=False).apply(lambda x: x.sample(min(len(x), s_size), random_state=42))\n",
    "        df_sampled = df_sampled.reset_index(drop=True)\n",
    "        print(f\"تعداد ردیف‌ها پس از نمونه‌برداری: {len(df_sampled)}\")\n",
    "    else:\n",
    "        df_sampled = df_filtered\n",
    "\n",
    "    print(\"\\nتوزیع نهایی داده‌ها بر اساس کلاس:\")\n",
    "    print(df_sampled[label_column].value_counts())\n",
    "    \n",
    "    return df_sampled\n",
    "\n",
    "df = load_and_filter_dataset(DATA_PATH, text_column=TEXT_COLUMN, label_column=LABEL_COLUMN, s_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>Label</th>\n",
       "      <th>CAT 1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I was diagnosed,about 10 years back with sever...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>Severe anxiety</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6824.0</td>\n",
       "      <td>dogheritage0</td>\n",
       "      <td>1.656847e+09</td>\n",
       "      <td>2022-07-03 21:12:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I don't want to wake up anyone but my whole lo...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>Please Help me, I feel like something bad is g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>the_beast69</td>\n",
       "      <td>1.603334e+09</td>\n",
       "      <td>2020-10-22 13:26:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This is my first time posting in this subreddi...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>Anxiety And Attachment Issues Ruining Relation...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6147.0</td>\n",
       "      <td>normalvibezonly</td>\n",
       "      <td>1.638653e+09</td>\n",
       "      <td>2021-12-05 08:20:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I started taking medication for my anxiety (Pr...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>Anxiety meds making me depressed, what should ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5053.0</td>\n",
       "      <td>Ghostinthemachinima</td>\n",
       "      <td>1.604996e+09</td>\n",
       "      <td>2020-11-10 19:12:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I survived a massive attack which lasted for t...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>I survived a massive attack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4349.0</td>\n",
       "      <td>horror_haller</td>\n",
       "      <td>1.623566e+09</td>\n",
       "      <td>2021-06-13 16:37:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                           selftext subreddit  \\\n",
       "0    1.0  I was diagnosed,about 10 years back with sever...   anxiety   \n",
       "1    1.0  I don't want to wake up anyone but my whole lo...   anxiety   \n",
       "2    1.0  This is my first time posting in this subreddi...   anxiety   \n",
       "3    1.0  I started taking medication for my anxiety (Pr...   anxiety   \n",
       "4    1.0  I survived a massive attack which lasted for t...   anxiety   \n",
       "\n",
       "                                               title Label CAT 1  Unnamed: 0  \\\n",
       "0                                     Severe anxiety   NaN   NaN      6824.0   \n",
       "1  Please Help me, I feel like something bad is g...   NaN   NaN      1820.0   \n",
       "2  Anxiety And Attachment Issues Ruining Relation...   NaN   NaN      6147.0   \n",
       "3  Anxiety meds making me depressed, what should ...   NaN   NaN      5053.0   \n",
       "4                        I survived a massive attack   NaN   NaN      4349.0   \n",
       "\n",
       "                author   created_utc            timestamp  \n",
       "0         dogheritage0  1.656847e+09  2022-07-03 21:12:54  \n",
       "1          the_beast69  1.603334e+09  2020-10-22 13:26:43  \n",
       "2      normalvibezonly  1.638653e+09  2021-12-05 08:20:03  \n",
       "3  Ghostinthemachinima  1.604996e+09  2020-11-10 19:12:19  \n",
       "4        horror_haller  1.623566e+09  2021-06-13 16:37:01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد ردیف‌ها پس از فیلتر کردن بر اساس لیبل: 1611011\n",
      "انجام نمونه‌برداری با s_size=8000 برای هر کلاس...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8888\\168613719.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_filtered.groupby(label_column, group_keys=False).apply(lambda x: x.sample(min(len(x), s_size), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تعداد ردیف‌ها پس از نمونه‌برداری: 40000\n",
      "\n",
      "توزیع نهایی داده‌ها بر اساس کلاس:\n",
      "subreddit\n",
      "anxiety         8000\n",
      "depression      8000\n",
      "lonely          8000\n",
      "mentalhealth    8000\n",
      "suicidewatch    8000\n",
      "Name: count, dtype: int64\n",
      "در حال ترکیب ستون‌های 'title' و 'selftext'...\n",
      "ترکیب با موفقیت انجام شد.\n"
     ]
    }
   ],
   "source": [
    "df = load_and_filter_dataset(DATA_PATH, text_column=TEXT_COLUMN, label_column=LABEL_COLUMN, s_size=8000)\n",
    "\n",
    "if not df.empty and 'title' in df.columns:\n",
    "    print(\"در حال ترکیب ستون‌های 'title' و 'selftext'...\")\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df[TEXT_COLUMN] = df['title'] + \" [SEP] \" + df[TEXT_COLUMN]\n",
    "    print(\"ترکیب با موفقیت انجام شد.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "39996    False\n",
       "39997    False\n",
       "39998    False\n",
       "39999    False\n",
       "40000    False\n",
       "Name: labels, Length: 40001, dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "کلاس‌های شناسایی شده برای آموزش: ['anxiety', 'depression', 'lonely', 'mentalhealth', 'suicidewatch']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████| 32000/32000 [00:17<00:00, 1809.97 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 8000/8000 [00:02<00:00, 2701.45 examples/s]\n",
      "Some weights of the model checkpoint at ./my_bert_model were not used when initializing BertForSequenceClassification: ['fit_denses.6.weight', 'fit_denses.2.bias', 'cls.seq_relationship.weight', 'fit_denses.4.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'fit_denses.3.bias', 'fit_denses.2.weight', 'fit_denses.5.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.0.bias', 'fit_denses.1.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.6.bias', 'fit_denses.0.weight', 'cls.predictions.bias', 'fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.5.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./my_bert_model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 32000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "شروع فرآیند fine-tuning مدل...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32000/40000 11:14:57 < 2:48:45, 0.79 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.333800</td>\n",
       "      <td>1.246762</td>\n",
       "      <td>0.473750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.117600</td>\n",
       "      <td>1.215304</td>\n",
       "      <td>0.483375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.026100</td>\n",
       "      <td>1.127699</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>1.115006</td>\n",
       "      <td>0.583375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.947900</td>\n",
       "      <td>1.101449</td>\n",
       "      <td>0.592250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>1.151170</td>\n",
       "      <td>0.583625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>1.204097</td>\n",
       "      <td>0.587250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>1.217763</td>\n",
       "      <td>0.584375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-4000\n",
      "Configuration saved in ./results_filtered\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-4000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-8000\n",
      "Configuration saved in ./results_filtered\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-8000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-12000\n",
      "Configuration saved in ./results_filtered\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-12000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-16000\n",
      "Configuration saved in ./results_filtered\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-16000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-20000\n",
      "Configuration saved in ./results_filtered\\checkpoint-20000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-24000\n",
      "Configuration saved in ./results_filtered\\checkpoint-24000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-24000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-28000\n",
      "Configuration saved in ./results_filtered\\checkpoint-28000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-28000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-28000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-28000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results_filtered\\checkpoint-32000\n",
      "Configuration saved in ./results_filtered\\checkpoint-32000\\config.json\n",
      "Model weights saved in ./results_filtered\\checkpoint-32000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_filtered\\checkpoint-32000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_filtered\\checkpoint-32000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_filtered\\checkpoint-20000 (score: 0.59225).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "آموزش با موفقیت به پایان رسید. ✅\n"
     ]
    }
   ],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df[LABEL_COLUMN])\n",
    "num_labels = len(le.classes_)\n",
    "id2label = {i: label for i, label in enumerate(le.classes_)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "print(f\"\\nکلاس‌های شناسایی شده برای آموزش: {list(le.classes_)}\")\n",
    "\n",
    "if not df.empty:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['labels'])\n",
    "\n",
    "    raw_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"eval\": Dataset.from_pandas(eval_df)\n",
    "    })\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[TEXT_COLUMN], truncation=True, max_length=256)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "    required_columns = ['input_ids', 'attention_mask', 'labels']\n",
    "    all_columns = tokenized_datasets[\"train\"].column_names\n",
    "    columns_to_remove = [col for col in all_columns if col not in required_columns]\n",
    "    if columns_to_remove:\n",
    "        tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
    "\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results_filtered',\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        fp16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"eval\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    print(\"\\nشروع فرآیند fine-tuning مدل...\")\n",
    "    trainer.train()\n",
    "    print(\"\\nآموزش با موفقیت به پایان رسید. \")\n",
    "else:\n",
    "    print(\"\\nدیتافریم خالی است. فرآیند آموزش متوقف شد.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch39)",
   "language": "python",
   "name": "torch39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
