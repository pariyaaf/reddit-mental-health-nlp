{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WvBgW7IkNbZg",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets version: 2.18.0\n",
      "huggingface_hub version: 0.20.3\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import huggingface_hub\n",
    "\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"huggingface_hub version:\", huggingface_hub.__version__)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "import wordninja\n",
    "#import spacy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import glob\n",
    "import language_tool_python\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 950M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # باید True باشد\n",
    "print(torch.cuda.get_device_name(0))  # نام GPU شما باید نمایش داده شود\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "1.13.1+cu117\n",
      "4.11.3\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 950M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from transformers import __version__\n",
    "print(__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3DK490TOWDg",
    "outputId": "2da03c3f-eedb-4858-d6d0-6de78cd1d363"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')                         \n",
    "nltk.download('stopwords')                     \n",
    "nltk.download('wordnet')                       \n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PbepnsaQNcRq"
   },
   "outputs": [],
   "source": [
    "def load_reddit_dataset(data_path, s_size=None, text_column=\"selftext\", label_column=\"subreddit\"):\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(data_path, \"**\", \"*.csv\"), recursive=True)\n",
    "    print(f\"Found {len(csv_files)} CSV files.\")\n",
    "\n",
    "    all_dataframes = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    all_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(\"Total rows before cleaning:\", len(all_data))\n",
    "\n",
    "    clear_data = all_data.dropna(subset=[text_column, label_column])\n",
    "    print(\"Total rows after dropping NA:\", len(clear_data))\n",
    "\n",
    "    if s_size is not None:\n",
    "        grouped = clear_data.groupby(label_column)\n",
    "        eligible_groups = [name for name, group in grouped if len(group) >= s_size]\n",
    "        filtered_data = clear_data[clear_data[label_column].isin(eligible_groups)]\n",
    "        df_sampled = filtered_data.groupby(label_column).sample(n=s_size, random_state=42)\n",
    "        df_sampled = df_sampled.reset_index(drop=True)\n",
    "        print(\"Sampled class distribution:\")\n",
    "        print(df_sampled[label_column].value_counts())\n",
    "    else:\n",
    "        df_sampled = clear_data\n",
    "        print(\"Returned all available data without sampling.\")\n",
    "        print(df_sampled[label_column].value_counts())\n",
    "\n",
    "    return df_sampled[text_column], df_sampled[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvBENdrOPrbH",
    "outputId": "37f70204-c966-42bb-fb20-4b69c37ec103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 223 CSV files.\n",
      "Total rows before cleaning: 1852403\n",
      "Total rows after dropping NA: 1797816\n",
      "Sampled class distribution:\n",
      "subreddit\n",
      "Anxiety         2000\n",
      "SuicideWatch    2000\n",
      "depression      2000\n",
      "lonely          2000\n",
      "mentalhealth    2000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "text_data, label_data = load_reddit_dataset(\"dataset\",s_size=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlu2alJcQHpe",
    "outputId": "573f5573-10f9-4535-e531-ff10c77d9fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few weeks ago, I finally caved and called my doctor to get back on my anxiety medicine after my anxiety got so bad that I was having panic attacks whenever I left the house. I cried picking it up. I was so ready to feel normal again. \n",
      "\n",
      "Well last night I went to the store and felt an anxiety attack coming. Instead of spiraling out of control like I have been, I was easily able to calm down. I can't share this with anyone else so I thought to post it here. I am so happy that I was able to finish shopping and that I didn't end up with a panic attack. \n",
      "\n",
      "I don't know why I waited so long to go back on my medicine.\n"
     ]
    }
   ],
   "source": [
    "print(text_data.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "lUSBv2m-NcT9",
    "outputId": "462afca7-3ac3-41e8-a2a7-d0154c135542"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A few weeks ago I finally caved and called my doctor to get back on my anxiety medicine after my anxiety got so bad that I was having panic attacks whenever I left the house I cried picking it up I was so ready to feel normal again \\n\\nWell last night I went to the store and felt an anxiety attack coming Instead of spiraling out of control like I have been I was easily able to calm down I cant share this with anyone else so I thought to post it here I am so happy that I was able to finish shopping and that I didnt end up with a panic attack \\n\\nI dont know why I waited so long to go back on my medicine'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    html_regex = re.compile(\"<.*?>\")\n",
    "    num_regex = re.compile(\"\\d+\")\n",
    "    pun_re = re.compile(\"[^\\w\\s]\")\n",
    "\n",
    "    cleaned_data = []\n",
    "\n",
    "    for text in data:\n",
    "        text = re.sub(html_regex, '', text)\n",
    "\n",
    "        urls = re.findall(r'https?://\\S+|www\\.\\S+', text)\n",
    "        for url in urls:\n",
    "            tokens = re.findall(r'[a-zA-Z]{4,}', url)\n",
    "            if len(tokens) >= 3:\n",
    "                text = text.replace(url, ' '.join(tokens))\n",
    "            else:\n",
    "                text = text.replace(url, '[URL]')\n",
    "\n",
    "        text = re.sub(num_regex, '', text)\n",
    "\n",
    "        text = re.sub(pun_re, '', text)\n",
    "\n",
    "        cleaned_data.append(text)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "\n",
    "text_data01 = clean_data(text_data)\n",
    "\n",
    "text_data01[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# def correct_text_languagetool(text):\n",
    "#     matches = tool.check(text)\n",
    "#     corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "#     return corrected_text\n",
    "\n",
    "# def correct_texts_languagetool(text_list):\n",
    "#     corrected_texts = []\n",
    "#     for text in text_list:\n",
    "#         corrected_text = correct_text_languagetool(text)\n",
    "#         corrected_texts.append(corrected_text)\n",
    "#     return corrected_texts\n",
    "\n",
    "# text_data_corrected = correct_texts_languagetool(text_data01)\n",
    "\n",
    "# print(text_data_corrected[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "h0r06_0dNcV7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'ago',\n",
       " 'I',\n",
       " 'finally',\n",
       " 'caved',\n",
       " 'and',\n",
       " 'called',\n",
       " 'my',\n",
       " 'doctor',\n",
       " 'to',\n",
       " 'get',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'medicine',\n",
       " 'after',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'got',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'that',\n",
       " 'I',\n",
       " 'was',\n",
       " 'having',\n",
       " 'panic',\n",
       " 'attacks',\n",
       " 'whenever',\n",
       " 'I',\n",
       " 'left',\n",
       " 'the',\n",
       " 'house',\n",
       " 'I',\n",
       " 'cried',\n",
       " 'picking',\n",
       " 'it',\n",
       " 'up',\n",
       " 'I',\n",
       " 'was',\n",
       " 'so',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'normal',\n",
       " 'again',\n",
       " 'Well',\n",
       " 'last',\n",
       " 'night',\n",
       " 'I',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " 'and',\n",
       " 'felt',\n",
       " 'an',\n",
       " 'anxiety',\n",
       " 'attack',\n",
       " 'coming',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'spiraling',\n",
       " 'out',\n",
       " 'of',\n",
       " 'control',\n",
       " 'like',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'I',\n",
       " 'was',\n",
       " 'easily',\n",
       " 'able',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " 'I',\n",
       " 'cant',\n",
       " 'share',\n",
       " 'this',\n",
       " 'with',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'so',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'post',\n",
       " 'it',\n",
       " 'here',\n",
       " 'I',\n",
       " 'am',\n",
       " 'so',\n",
       " 'happy',\n",
       " 'that',\n",
       " 'I',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'shopping',\n",
       " 'and',\n",
       " 'that',\n",
       " 'I',\n",
       " 'didnt',\n",
       " 'end',\n",
       " 'up',\n",
       " 'with',\n",
       " 'a',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'why',\n",
       " 'I',\n",
       " 'waited',\n",
       " 'so',\n",
       " 'long',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splited_data(data):\n",
    "    temp_data = []\n",
    "    for corpus in data:\n",
    "        words = \" \".join(corpus.split()).split()\n",
    "        fixed_words = []\n",
    "\n",
    "        for word in words:\n",
    "            if len(word) > 30:\n",
    "                split_words = wordninja.split(word)\n",
    "                fixed_words.extend(split_words)\n",
    "            else:\n",
    "                fixed_words.append(word)\n",
    "\n",
    "        temp_data.append(fixed_words)\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "\n",
    "# text_data02 = splited_data(text_data_corrected)\n",
    "text_data02 = splited_data(text_data01)\n",
    "\n",
    "text_data02[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dWDV41I-NcYV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'finally',\n",
       " 'caved',\n",
       " 'and',\n",
       " 'called',\n",
       " 'my',\n",
       " 'doctor',\n",
       " 'to',\n",
       " 'get',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'medicine',\n",
       " 'after',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'got',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'that',\n",
       " 'i',\n",
       " 'was',\n",
       " 'having',\n",
       " 'panic',\n",
       " 'attacks',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'left',\n",
       " 'the',\n",
       " 'house',\n",
       " 'i',\n",
       " 'cried',\n",
       " 'picking',\n",
       " 'it',\n",
       " 'up',\n",
       " 'i',\n",
       " 'was',\n",
       " 'so',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'normal',\n",
       " 'again',\n",
       " 'well',\n",
       " 'last',\n",
       " 'night',\n",
       " 'i',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " 'and',\n",
       " 'felt',\n",
       " 'an',\n",
       " 'anxiety',\n",
       " 'attack',\n",
       " 'coming',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'spiraling',\n",
       " 'out',\n",
       " 'of',\n",
       " 'control',\n",
       " 'like',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'i',\n",
       " 'was',\n",
       " 'easily',\n",
       " 'able',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'share',\n",
       " 'this',\n",
       " 'with',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'so',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'post',\n",
       " 'it',\n",
       " 'here',\n",
       " 'i',\n",
       " 'am',\n",
       " 'so',\n",
       " 'happy',\n",
       " 'that',\n",
       " 'i',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'shopping',\n",
       " 'and',\n",
       " 'that',\n",
       " 'i',\n",
       " 'didnt',\n",
       " 'end',\n",
       " 'up',\n",
       " 'with',\n",
       " 'a',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'why',\n",
       " 'i',\n",
       " 'waited',\n",
       " 'so',\n",
       " 'long',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase(data):\n",
    "    data_lowercase = []\n",
    "\n",
    "    for corpus in data:\n",
    "        temp_list = []\n",
    "        for word in corpus:\n",
    "            temp = word.lower()\n",
    "            temp_list.append(temp)\n",
    "\n",
    "        data_lowercase.append(temp_list)\n",
    "\n",
    "    return data_lowercase\n",
    "\n",
    "text_data03 = lowercase(text_data02)\n",
    "\n",
    "\n",
    "text_data03[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "XRfta8aMNca5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weeks',\n",
       " 'ago',\n",
       " 'finally',\n",
       " 'caved',\n",
       " 'called',\n",
       " 'doctor',\n",
       " 'get',\n",
       " 'back',\n",
       " 'anxiety',\n",
       " 'medicine',\n",
       " 'anxiety',\n",
       " 'got',\n",
       " 'bad',\n",
       " 'panic',\n",
       " 'attacks',\n",
       " 'whenever',\n",
       " 'left',\n",
       " 'house',\n",
       " 'cried',\n",
       " 'picking',\n",
       " 'ready',\n",
       " 'feel',\n",
       " 'normal',\n",
       " 'well',\n",
       " 'last',\n",
       " 'night',\n",
       " 'went',\n",
       " 'store',\n",
       " 'felt',\n",
       " 'anxiety',\n",
       " 'attack',\n",
       " 'coming',\n",
       " 'instead',\n",
       " 'spiraling',\n",
       " 'control',\n",
       " 'like',\n",
       " 'easily',\n",
       " 'able',\n",
       " 'calm',\n",
       " 'cant',\n",
       " 'share',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'thought',\n",
       " 'post',\n",
       " 'happy',\n",
       " 'able',\n",
       " 'finish',\n",
       " 'shopping',\n",
       " 'didnt',\n",
       " 'end',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'waited',\n",
       " 'long',\n",
       " 'go',\n",
       " 'back',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(data):\n",
    "\n",
    "    res_data = []\n",
    "    for corpus in data:\n",
    "        temp_list=[]\n",
    "        for word in corpus:\n",
    "            if word not in stopwords.words(\"english\") :\n",
    "                temp_list.append(word)\n",
    "\n",
    "        res_data.append(temp_list)\n",
    "\n",
    "    return res_data\n",
    "text_data04 = remove_stopwords(text_data03)\n",
    "\n",
    "\n",
    "\n",
    "text_data04[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(eval_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Htv92vJONceY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weeks', 'Ġago', 'Ġfinally', 'Ġcaved', 'Ġcalled', 'Ġdoctor', 'Ġget', 'Ġback', 'Ġanxiety', 'Ġmedicine', 'Ġanxiety', 'Ġgot', 'Ġbad', 'Ġpanic', 'Ġattacks', 'Ġwhenever', 'Ġleft', 'Ġhouse', 'Ġcried', 'Ġpicking', 'Ġready', 'Ġfeel', 'Ġnormal', 'Ġwell', 'Ġlast', 'Ġnight', 'Ġwent', 'Ġstore', 'Ġfelt', 'Ġanxiety', 'Ġattack', 'Ġcoming', 'Ġinstead', 'Ġspiraling', 'Ġcontrol', 'Ġlike', 'Ġeasily', 'Ġable', 'Ġcalm', 'Ġcant', 'Ġshare', 'Ġanyone', 'Ġelse', 'Ġthought', 'Ġpost', 'Ġhappy', 'Ġable', 'Ġfinish', 'Ġshopping', 'Ġdidnt', 'Ġend', 'Ġpanic', 'Ġattack', 'Ġdont', 'Ġknow', 'Ġwaited', 'Ġlong', 'Ġgo', 'Ġback', 'Ġmedicine']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "def bpe_encode_corpus(data, vocab_size=30000, min_frequency=2,\n",
    "                      special_tokens=None):\n",
    "    \"\"\"\n",
    "    data: لیستی از لیست‌های توکن (مثل text_data04)\n",
    "    خروجی: لیست توکن‌های BPE برای هر جمله\n",
    "    \"\"\"\n",
    "    if special_tokens is None:\n",
    "        special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "    # 1. آماده‌سازی رشته‌ها\n",
    "    raw_texts = [\" \".join(tokens) for tokens in data]\n",
    "\n",
    "    # 2. ساخت و آموزش ByteLevelBPETokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train_from_iterator(\n",
    "        raw_texts,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=special_tokens\n",
    "    )\n",
    "\n",
    "    # 3. کدگذاری به BPE\n",
    "    bpe_encoded = []\n",
    "    for text in raw_texts:\n",
    "        enc = tokenizer.encode(text)\n",
    "        bpe_encoded.append(enc.tokens)\n",
    "\n",
    "    return bpe_encoded\n",
    "\n",
    "# استفاده:\n",
    "bpe_texts = bpe_encode_corpus(text_data04)\n",
    "\n",
    "# توکن‌های BPE جمله‌ی اول:\n",
    "print(bpe_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at my_bert_model were not used when initializing BertForSequenceClassification: ['fit_denses.1.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.6.weight', 'fit_denses.3.bias', 'fit_denses.0.weight', 'fit_denses.3.weight', 'fit_denses.0.bias', 'fit_denses.5.bias', 'fit_denses.2.weight', 'fit_denses.4.weight', 'fit_denses.4.bias', 'cls.seq_relationship.weight', 'fit_denses.5.weight', 'fit_denses.2.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'fit_denses.6.bias', 'fit_denses.1.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at my_bert_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "\n",
    "# اگر label_data لیست لیبل‌هاست:\n",
    "le = LabelEncoder()\n",
    "le.fit(label_data)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"my_bert_model\",\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEzuAAV9Viqt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 8000/8000 [00:01<00:00, 4717.55 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 3650.46 examples/s]\n",
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 3:47:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.651300</td>\n",
       "      <td>1.613843</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.613600</td>\n",
       "      <td>1.609746</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.063341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.610800</td>\n",
       "      <td>1.610295</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.067586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.610100</td>\n",
       "      <td>1.609637</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.609600</td>\n",
       "      <td>1.609649</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=1.6190668212890624, metrics={'train_runtime': 13661.3339, 'train_samples_per_second': 2.928, 'train_steps_per_second': 0.183, 'total_flos': 1324744857600000.0, 'train_loss': 1.6190668212890624, 'epoch': 5.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "# ۱. بارگذاری توکنایزر\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ۲. تقسیم داده‌ها به آموزش و ارزیابی\n",
    "splits = dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "train_dataset = splits[\"train\"]\n",
    "eval_dataset  = splits[\"test\"]\n",
    "\n",
    "# ۳. تغییر نام ستون 'label' به 'labels' تا با Trainer هماهنگ شود\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "eval_dataset  = eval_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# ۴. توکنیزه کردن داده‌ها و اضافه کردن 'input_ids' و 'attention_mask'\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "eval_dataset  = eval_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# ۵. حذف ستون 'text' از داده‌ها پس از توکنایز کردن\n",
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "eval_dataset  = eval_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# ۶. تبدیل داده‌ها به فرمت torch.Tensor برای Trainer\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# ۷. اطمینان از قرارگیری مدل روی CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ۸. Data collator برای padding پویا\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# ۹. تنظیمات آموزش با حذف GPU و fp16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.0,\n",
    "    logging_dir=\"./logs\",\n",
    "    no_cuda=True,   # اجبار به استفاده از CPU\n",
    "    fp16=False,     # غیرفعال کردن mixed-precision\n",
    ")\n",
    "\n",
    "# ۱۰. ساخت Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ۱۱. شروع فاین‌تونینگ head\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "VZUVvhR-xRFh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 7:43:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_bert_model_head_finetuned\n",
      "Configuration saved in my_bert_model_head_finetuned\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results: {'eval_loss': 1.6096493005752563, 'eval_accuracy': 0.1955, 'eval_f1': 0.06394019238812212, 'eval_runtime': 202.7427, 'eval_samples_per_second': 9.865, 'eval_steps_per_second': 0.617, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_bert_model_head_finetuned\\pytorch_model.bin\n",
      "tokenizer config file saved in my_bert_model_head_finetuned\\tokenizer_config.json\n",
      "Special tokens file saved in my_bert_model_head_finetuned\\special_tokens_map.json\n",
      "tokenizer config file saved in my_bert_model_head_finetuned\\tokenizer_config.json\n",
      "Special tokens file saved in my_bert_model_head_finetuned\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('my_bert_model_head_finetuned\\\\tokenizer_config.json',\n",
       " 'my_bert_model_head_finetuned\\\\special_tokens_map.json',\n",
       " 'my_bert_model_head_finetuned\\\\vocab.txt',\n",
       " 'my_bert_model_head_finetuned\\\\added_tokens.json',\n",
       " 'my_bert_model_head_finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ۱۰. ارزیابی نهایی\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Eval results:\", eval_results)\n",
    "\n",
    "# ۱۱. (اختیاری) ذخیرهٔ مدل فاین‌تون‌شده\n",
    "trainer.save_model(\"my_bert_model_head_finetuned\")\n",
    "tokenizer.save_pretrained(\"my_bert_model_head_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "5laoEfJ-Csm5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 1:58:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.609600</td>\n",
       "      <td>1.609649</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.609500</td>\n",
       "      <td>1.609649</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.609400</td>\n",
       "      <td>1.609649</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.609500</td>\n",
       "      <td>1.609649</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.609300</td>\n",
       "      <td>1.609649</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.063940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=1.6094640380859375, metrics={'train_runtime': 7121.1941, 'train_samples_per_second': 5.617, 'train_steps_per_second': 0.351, 'total_flos': 1324744857600000.0, 'train_loss': 1.6094640380859375, 'epoch': 5.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# اینچا داره میگه و بررسی میگه اگه فریز نگنیم چقدر دقت میشه2 تا لایه اخر ر\n",
    "# 1. Unfreeze دو لایهٔ آخر Transformer\n",
    "for name, param in model.bert.named_parameters():\n",
    "    # لایه‌های encoder.layer.10 و encoder.layer.11 را باز کنید\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# 2. نرخ یادگیری کمتری انتخاب کنید\n",
    "training_args.learning_rate = 5e-5\n",
    "\n",
    "# 3. full model fine-tuning\n",
    "# trainer را دوباره با همین تنظیمات اجرا کنید.\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "# # ۶. Data collator برای padding پویا\n",
    "# data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# # ۷. تعیین hyper-parameters\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",     # ارزیابی در پایان هر Epoch\n",
    "#     save_strategy=\"epoch\",           # ذخیره مدل در پایان هر Epoch\n",
    "#     learning_rate=2e-3,              # lr بالاتر چون فقط head را آپدیت می‌کنیم\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.0,                # معمولاً برای head decay=0 مناسب است\n",
    "#     logging_dir=\"./logs\",\n",
    "#     fp16=True,                       # mixed-precision برای سرعت بالاتر روی GPU\n",
    "# )\n",
    "\n",
    "# # ۸. ساخت Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                     # مدل بارگذاری‌شده و encoder فریزشده\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics, # تابع محاسبه متریک‌ها\n",
    "# )\n",
    "\n",
    "# # ۹. شروع فاین‌تونینگ head\n",
    "# trainer.train()  همه رو بزار روی سی \\ی یو"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch39)",
   "language": "python",
   "name": "torch39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
