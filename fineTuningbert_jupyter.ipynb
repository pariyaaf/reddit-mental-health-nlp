{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WvBgW7IkNbZg"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "import joblib\n",
    "import wordninja\n",
    "import spacy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import glob\n",
    "import language_tool_python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 950M\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3DK490TOWDg",
    "outputId": "2da03c3f-eedb-4858-d6d0-6de78cd1d363"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')                         \n",
    "nltk.download('stopwords')                     \n",
    "nltk.download('wordnet')                       \n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PbepnsaQNcRq"
   },
   "outputs": [],
   "source": [
    "def load_reddit_dataset(data_path, s_size=None, text_column=\"selftext\", label_column=\"subreddit\"):\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(data_path, \"**\", \"*.csv\"), recursive=True)\n",
    "    print(f\"Found {len(csv_files)} CSV files.\")\n",
    "\n",
    "    all_dataframes = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    all_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(\"Total rows before cleaning:\", len(all_data))\n",
    "\n",
    "    clear_data = all_data.dropna(subset=[text_column, label_column])\n",
    "    print(\"Total rows after dropping NA:\", len(clear_data))\n",
    "\n",
    "    if s_size is not None:\n",
    "        grouped = clear_data.groupby(label_column)\n",
    "        eligible_groups = [name for name, group in grouped if len(group) >= s_size]\n",
    "        filtered_data = clear_data[clear_data[label_column].isin(eligible_groups)]\n",
    "        df_sampled = filtered_data.groupby(label_column).sample(n=s_size, random_state=42)\n",
    "        df_sampled = df_sampled.reset_index(drop=True)\n",
    "        print(\"Sampled class distribution:\")\n",
    "        print(df_sampled[label_column].value_counts())\n",
    "    else:\n",
    "        df_sampled = clear_data\n",
    "        print(\"Returned all available data without sampling.\")\n",
    "        print(df_sampled[label_column].value_counts())\n",
    "\n",
    "    return df_sampled[text_column], df_sampled[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvBENdrOPrbH",
    "outputId": "37f70204-c966-42bb-fb20-4b69c37ec103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 223 CSV files.\n",
      "Total rows before cleaning: 1852403\n",
      "Total rows after dropping NA: 1797816\n",
      "Sampled class distribution:\n",
      "subreddit\n",
      "Anxiety         100\n",
      "SuicideWatch    100\n",
      "depression      100\n",
      "lonely          100\n",
      "mentalhealth    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "text_data, label_data = load_reddit_dataset(\"dataset\",s_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlu2alJcQHpe",
    "outputId": "573f5573-10f9-4535-e531-ff10c77d9fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few weeks ago, I finally caved and called my doctor to get back on my anxiety medicine after my anxiety got so bad that I was having panic attacks whenever I left the house. I cried picking it up. I was so ready to feel normal again. \n",
      "\n",
      "Well last night I went to the store and felt an anxiety attack coming. Instead of spiraling out of control like I have been, I was easily able to calm down. I can't share this with anyone else so I thought to post it here. I am so happy that I was able to finish shopping and that I didn't end up with a panic attack. \n",
      "\n",
      "I don't know why I waited so long to go back on my medicine.\n"
     ]
    }
   ],
   "source": [
    "print(text_data.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "lUSBv2m-NcT9",
    "outputId": "462afca7-3ac3-41e8-a2a7-d0154c135542"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A few weeks ago I finally caved and called my doctor to get back on my anxiety medicine after my anxiety got so bad that I was having panic attacks whenever I left the house I cried picking it up I was so ready to feel normal again \\n\\nWell last night I went to the store and felt an anxiety attack coming Instead of spiraling out of control like I have been I was easily able to calm down I cant share this with anyone else so I thought to post it here I am so happy that I was able to finish shopping and that I didnt end up with a panic attack \\n\\nI dont know why I waited so long to go back on my medicine'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    html_regex = re.compile(\"<.*?>\")\n",
    "    num_regex = re.compile(\"\\d+\")\n",
    "    pun_re = re.compile(\"[^\\w\\s]\")\n",
    "\n",
    "    cleaned_data = []\n",
    "\n",
    "    for text in data:\n",
    "        text = re.sub(html_regex, '', text)\n",
    "\n",
    "        urls = re.findall(r'https?://\\S+|www\\.\\S+', text)\n",
    "        for url in urls:\n",
    "            tokens = re.findall(r'[a-zA-Z]{4,}', url)\n",
    "            if len(tokens) >= 3:\n",
    "                text = text.replace(url, ' '.join(tokens))\n",
    "            else:\n",
    "                text = text.replace(url, '[URL]')\n",
    "\n",
    "        text = re.sub(num_regex, '', text)\n",
    "\n",
    "        text = re.sub(pun_re, '', text)\n",
    "\n",
    "        cleaned_data.append(text)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "\n",
    "text_data01 = clean_data(text_data)\n",
    "\n",
    "text_data01[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few weeks ago I finally caved and called my doctor to get back on my anxiety medicine after my anxiety got so bad that I was having panic attacks whenever I left the house I cried picking it up I was so ready to feel normal again \n",
      "\n",
      "Well last night I went to the store and felt an anxiety attack coming Instead of spiraling out of control like I have been I was easily able to calm down I can't share this with anyone else, so I thought to post it here I am so happy that I was able to finish shopping and that I didn't end up with a panic attack \n",
      "\n",
      "I don't know why I waited so long to go back on my medicine\n"
     ]
    }
   ],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def correct_text_languagetool(text):\n",
    "    matches = tool.check(text)\n",
    "    corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "    return corrected_text\n",
    "\n",
    "def correct_texts_languagetool(text_list):\n",
    "    corrected_texts = []\n",
    "    for text in text_list:\n",
    "        corrected_text = correct_text_languagetool(text)\n",
    "        corrected_texts.append(corrected_text)\n",
    "    return corrected_texts\n",
    "\n",
    "text_data_corrected = correct_texts_languagetool(text_data01)\n",
    "\n",
    "print(text_data_corrected[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h0r06_0dNcV7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'ago',\n",
       " 'I',\n",
       " 'finally',\n",
       " 'caved',\n",
       " 'and',\n",
       " 'called',\n",
       " 'my',\n",
       " 'doctor',\n",
       " 'to',\n",
       " 'get',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'medicine',\n",
       " 'after',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'got',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'that',\n",
       " 'I',\n",
       " 'was',\n",
       " 'having',\n",
       " 'panic',\n",
       " 'attacks',\n",
       " 'whenever',\n",
       " 'I',\n",
       " 'left',\n",
       " 'the',\n",
       " 'house',\n",
       " 'I',\n",
       " 'cried',\n",
       " 'picking',\n",
       " 'it',\n",
       " 'up',\n",
       " 'I',\n",
       " 'was',\n",
       " 'so',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'normal',\n",
       " 'again',\n",
       " 'Well',\n",
       " 'last',\n",
       " 'night',\n",
       " 'I',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " 'and',\n",
       " 'felt',\n",
       " 'an',\n",
       " 'anxiety',\n",
       " 'attack',\n",
       " 'coming',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'spiraling',\n",
       " 'out',\n",
       " 'of',\n",
       " 'control',\n",
       " 'like',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'I',\n",
       " 'was',\n",
       " 'easily',\n",
       " 'able',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " 'I',\n",
       " \"can't\",\n",
       " 'share',\n",
       " 'this',\n",
       " 'with',\n",
       " 'anyone',\n",
       " 'else,',\n",
       " 'so',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'post',\n",
       " 'it',\n",
       " 'here',\n",
       " 'I',\n",
       " 'am',\n",
       " 'so',\n",
       " 'happy',\n",
       " 'that',\n",
       " 'I',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'shopping',\n",
       " 'and',\n",
       " 'that',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'end',\n",
       " 'up',\n",
       " 'with',\n",
       " 'a',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'why',\n",
       " 'I',\n",
       " 'waited',\n",
       " 'so',\n",
       " 'long',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splited_data(data):\n",
    "    temp_data = []\n",
    "    for corpus in data:\n",
    "        words = \" \".join(corpus.split()).split()\n",
    "        fixed_words = []\n",
    "\n",
    "        for word in words:\n",
    "            if len(word) > 30:\n",
    "                split_words = wordninja.split(word)\n",
    "                fixed_words.extend(split_words)\n",
    "            else:\n",
    "                fixed_words.append(word)\n",
    "\n",
    "        temp_data.append(fixed_words)\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "\n",
    "text_data02 = splited_data(text_data_corrected)\n",
    "\n",
    "text_data02[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dWDV41I-NcYV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'finally',\n",
       " 'caved',\n",
       " 'and',\n",
       " 'called',\n",
       " 'my',\n",
       " 'doctor',\n",
       " 'to',\n",
       " 'get',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'medicine',\n",
       " 'after',\n",
       " 'my',\n",
       " 'anxiety',\n",
       " 'got',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'that',\n",
       " 'i',\n",
       " 'was',\n",
       " 'having',\n",
       " 'panic',\n",
       " 'attacks',\n",
       " 'whenever',\n",
       " 'i',\n",
       " 'left',\n",
       " 'the',\n",
       " 'house',\n",
       " 'i',\n",
       " 'cried',\n",
       " 'picking',\n",
       " 'it',\n",
       " 'up',\n",
       " 'i',\n",
       " 'was',\n",
       " 'so',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'normal',\n",
       " 'again',\n",
       " 'well',\n",
       " 'last',\n",
       " 'night',\n",
       " 'i',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " 'and',\n",
       " 'felt',\n",
       " 'an',\n",
       " 'anxiety',\n",
       " 'attack',\n",
       " 'coming',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'spiraling',\n",
       " 'out',\n",
       " 'of',\n",
       " 'control',\n",
       " 'like',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'i',\n",
       " 'was',\n",
       " 'easily',\n",
       " 'able',\n",
       " 'to',\n",
       " 'calm',\n",
       " 'down',\n",
       " 'i',\n",
       " \"can't\",\n",
       " 'share',\n",
       " 'this',\n",
       " 'with',\n",
       " 'anyone',\n",
       " 'else,',\n",
       " 'so',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'post',\n",
       " 'it',\n",
       " 'here',\n",
       " 'i',\n",
       " 'am',\n",
       " 'so',\n",
       " 'happy',\n",
       " 'that',\n",
       " 'i',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'shopping',\n",
       " 'and',\n",
       " 'that',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'end',\n",
       " 'up',\n",
       " 'with',\n",
       " 'a',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'why',\n",
       " 'i',\n",
       " 'waited',\n",
       " 'so',\n",
       " 'long',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back',\n",
       " 'on',\n",
       " 'my',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase(data):\n",
    "    data_lowercase = []\n",
    "\n",
    "    for corpus in data:\n",
    "        temp_list = []\n",
    "        for word in corpus:\n",
    "            temp = word.lower()\n",
    "            temp_list.append(temp)\n",
    "\n",
    "        data_lowercase.append(temp_list)\n",
    "\n",
    "    return data_lowercase\n",
    "\n",
    "text_data03 = lowercase(text_data02)\n",
    "\n",
    "\n",
    "text_data03[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XRfta8aMNca5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weeks',\n",
       " 'ago',\n",
       " 'finally',\n",
       " 'caved',\n",
       " 'called',\n",
       " 'doctor',\n",
       " 'get',\n",
       " 'back',\n",
       " 'anxiety',\n",
       " 'medicine',\n",
       " 'anxiety',\n",
       " 'got',\n",
       " 'bad',\n",
       " 'panic',\n",
       " 'attacks',\n",
       " 'whenever',\n",
       " 'left',\n",
       " 'house',\n",
       " 'cried',\n",
       " 'picking',\n",
       " 'ready',\n",
       " 'feel',\n",
       " 'normal',\n",
       " 'well',\n",
       " 'last',\n",
       " 'night',\n",
       " 'went',\n",
       " 'store',\n",
       " 'felt',\n",
       " 'anxiety',\n",
       " 'attack',\n",
       " 'coming',\n",
       " 'instead',\n",
       " 'spiraling',\n",
       " 'control',\n",
       " 'like',\n",
       " 'easily',\n",
       " 'able',\n",
       " 'calm',\n",
       " \"can't\",\n",
       " 'share',\n",
       " 'anyone',\n",
       " 'else,',\n",
       " 'thought',\n",
       " 'post',\n",
       " 'happy',\n",
       " 'able',\n",
       " 'finish',\n",
       " 'shopping',\n",
       " 'end',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'know',\n",
       " 'waited',\n",
       " 'long',\n",
       " 'go',\n",
       " 'back',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(data):\n",
    "\n",
    "    res_data = []\n",
    "    for corpus in data:\n",
    "        temp_list=[]\n",
    "        for word in corpus:\n",
    "            if word not in stopwords.words(\"english\") :\n",
    "                temp_list.append(word)\n",
    "\n",
    "        res_data.append(temp_list)\n",
    "\n",
    "    return res_data\n",
    "text_data04 = remove_stopwords(text_data03)\n",
    "\n",
    "\n",
    "\n",
    "text_data04[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Htv92vJONceY"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_spacy(data):\n",
    "  \n",
    "    pos_list = []\n",
    "    for tokens in data:\n",
    "        doc = nlp(\" \".join(tokens))\n",
    "        pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "        pos_list.append(pos_tags)\n",
    "    return pos_list\n",
    "\n",
    "\n",
    "text_data06 = pos_spacy(text_data04)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AkMG8cll3i1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weeks', 'NOUN'),\n",
       " ('ago', 'ADV'),\n",
       " ('finally', 'ADV'),\n",
       " ('caved', 'VERB'),\n",
       " ('called', 'VERB'),\n",
       " ('doctor', 'NOUN'),\n",
       " ('get', 'VERB'),\n",
       " ('back', 'ADV'),\n",
       " ('anxiety', 'NOUN'),\n",
       " ('medicine', 'NOUN'),\n",
       " ('anxiety', 'NOUN'),\n",
       " ('got', 'VERB'),\n",
       " ('bad', 'ADJ'),\n",
       " ('panic', 'NOUN'),\n",
       " ('attacks', 'NOUN'),\n",
       " ('whenever', 'SCONJ'),\n",
       " ('left', 'ADJ'),\n",
       " ('house', 'PROPN'),\n",
       " ('cried', 'VERB'),\n",
       " ('picking', 'VERB'),\n",
       " ('ready', 'ADJ'),\n",
       " ('feel', 'VERB'),\n",
       " ('normal', 'ADJ'),\n",
       " ('well', 'ADV'),\n",
       " ('last', 'ADJ'),\n",
       " ('night', 'NOUN'),\n",
       " ('went', 'VERB'),\n",
       " ('store', 'NOUN'),\n",
       " ('felt', 'VERB'),\n",
       " ('anxiety', 'NOUN'),\n",
       " ('attack', 'NOUN'),\n",
       " ('coming', 'VERB'),\n",
       " ('instead', 'ADV'),\n",
       " ('spiraling', 'VERB'),\n",
       " ('control', 'NOUN'),\n",
       " ('like', 'ADP'),\n",
       " ('easily', 'ADV'),\n",
       " ('able', 'ADJ'),\n",
       " ('calm', 'NOUN'),\n",
       " ('ca', 'AUX'),\n",
       " (\"n't\", 'PART'),\n",
       " ('share', 'VERB'),\n",
       " ('anyone', 'PRON'),\n",
       " ('else', 'ADV'),\n",
       " (',', 'PUNCT'),\n",
       " ('thought', 'VERB'),\n",
       " ('post', 'VERB'),\n",
       " ('happy', 'ADJ'),\n",
       " ('able', 'ADJ'),\n",
       " ('finish', 'NOUN'),\n",
       " ('shopping', 'NOUN'),\n",
       " ('end', 'NOUN'),\n",
       " ('panic', 'NOUN'),\n",
       " ('attack', 'NOUN'),\n",
       " ('know', 'VERB'),\n",
       " ('waited', 'VERB'),\n",
       " ('long', 'ADV'),\n",
       " ('go', 'VERB'),\n",
       " ('back', 'ADV'),\n",
       " ('medicine', 'NOUN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data06[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YEzuAAV9Viqt"
   },
   "outputs": [],
   "source": [
    "def fix_negations(data):\n",
    "    fixed_data = []\n",
    "    negation_starters = ['ca', 'do', 'did', 'wo', 'sha', 'is', 'was', 'would', 'could', 'should', 'might', 'must']\n",
    "\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if (i + 1 < len(data) and\n",
    "            data[i][0].lower() in negation_starters and\n",
    "            data[i+1][0].lower() == \"n't\"):\n",
    "            \n",
    "            combined_word = data[i][0] + \"n't\"\n",
    "            fixed_data.append((combined_word, data[i][1]))\n",
    "            i += 2\n",
    "        else:\n",
    "            fixed_data.append(data[i])\n",
    "            i += 1\n",
    "\n",
    "    return fixed_data\n",
    "\n",
    "def lemmatizing(data):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatize_data = []\n",
    "\n",
    "    tag = {\n",
    "        \"NOUN\": \"n\",\n",
    "        \"VERB\": \"v\",\n",
    "        \"ADJ\": \"a\",\n",
    "        \"ADV\": \"r\"\n",
    "    }\n",
    "\n",
    "    for corpus in data:\n",
    "        corpus_fixed = fix_negations(corpus)\n",
    "        temp_list = []\n",
    "\n",
    "        for word in corpus_fixed:\n",
    "            pos_tag = tag.get(word[1], None)\n",
    "            if pos_tag:\n",
    "                lemma = wnl.lemmatize(word[0], pos=pos_tag)\n",
    "            else:\n",
    "                lemma = word[0]\n",
    "            temp_list.append(lemma)\n",
    "\n",
    "        lemmatize_data.append(temp_list)\n",
    "\n",
    "    return lemmatize_data\n",
    "\n",
    "text_data07 = lemmatizing(text_data06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VZUVvhR-xRFh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week',\n",
       " 'ago',\n",
       " 'finally',\n",
       " 'cave',\n",
       " 'call',\n",
       " 'doctor',\n",
       " 'get',\n",
       " 'back',\n",
       " 'anxiety',\n",
       " 'medicine',\n",
       " 'anxiety',\n",
       " 'get',\n",
       " 'bad',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'whenever',\n",
       " 'left',\n",
       " 'house',\n",
       " 'cry',\n",
       " 'pick',\n",
       " 'ready',\n",
       " 'feel',\n",
       " 'normal',\n",
       " 'well',\n",
       " 'last',\n",
       " 'night',\n",
       " 'go',\n",
       " 'store',\n",
       " 'felt',\n",
       " 'anxiety',\n",
       " 'attack',\n",
       " 'come',\n",
       " 'instead',\n",
       " 'spiral',\n",
       " 'control',\n",
       " 'like',\n",
       " 'easily',\n",
       " 'able',\n",
       " 'calm',\n",
       " \"can't\",\n",
       " 'share',\n",
       " 'anyone',\n",
       " 'else',\n",
       " ',',\n",
       " 'think',\n",
       " 'post',\n",
       " 'happy',\n",
       " 'able',\n",
       " 'finish',\n",
       " 'shopping',\n",
       " 'end',\n",
       " 'panic',\n",
       " 'attack',\n",
       " 'know',\n",
       " 'wait',\n",
       " 'long',\n",
       " 'go',\n",
       " 'back',\n",
       " 'medicine']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data07[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5laoEfJ-Csm5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./my_bert_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b04fa1917548f882b0647cbe1a3fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17512\\300934201.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 28:07:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.628119</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.573067</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.573848</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.57306706905365, 'eval_accuracy': 0.27, 'eval_runtime': 55.8042, 'eval_samples_per_second': 1.792, 'eval_steps_per_second': 0.125, 'epoch': 3.0}\n",
      "Class names: ['Anxiety' 'SuicideWatch' 'depression' 'lonely' 'mentalhealth']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# تبدیل لیبل‌ها از رشته به عدد\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(label_data)\n",
    "\n",
    "# تبدیل توکن‌های پیش‌پردازش شده به جملات رشته‌ای\n",
    "texts_for_bert = [\" \".join(tokens) for tokens in text_data07]\n",
    "\n",
    "# ساخت دیتاست\n",
    "data_dict = {'text': texts_for_bert, 'label': labels_encoded}\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# بارگذاری tokenizer و مدل BERT از فایل محلی یا اسم مدل Huggingface\n",
    "model_path = \"./my_bert_model\"  # یا مدل مثل \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "num_classes = len(le.classes_)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_classes)\n",
    "\n",
    "# تابع توکنیزه کردن با padding و truncation\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# تقسیم داده‌ها\n",
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# تعریف متریک ارزیابی\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# تنظیمات آموزش\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# ساخت Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# آموزش مدل\n",
    "trainer.train()\n",
    "\n",
    "# ارزیابی نهایی\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# برای نمایش نام کلاس‌ها\n",
    "print(\"Class names:\", le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           Anxiety\n",
      "1           Anxiety\n",
      "2           Anxiety\n",
      "3           Anxiety\n",
      "4           Anxiety\n",
      "           ...     \n",
      "495    mentalhealth\n",
      "496    mentalhealth\n",
      "497    mentalhealth\n",
      "498    mentalhealth\n",
      "499    mentalhealth\n",
      "Name: subreddit, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مدل روی دستگاه: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mمدل روی دستگاه: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# انتقال مدل به دستگاه انتخاب شده (GPU یا CPU)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# انتقال ورودی‌ها به دستگاه انتخاب شده\u001b[39;00m\n\u001b[0;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "11.7\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n",
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
